{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mhuertas/Error/settings.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Inputs/Duplications.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-63776aeb1e21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m#DataFrame showing all the duplications provided by OrthoFinder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mDUPLICATIONS_DF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDuplications_File\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m###############################################################################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Inputs/Duplications.tsv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "from Bio import Phylo\n",
    "from Bio import SeqIO\n",
    "import ete3\n",
    "from ete3 import Tree\n",
    "import os.path as path\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "### First we need to get the information from a \"Settings_File\". There we have all the paths of all the files we are going to need. \n",
    "Settings_File = input()\n",
    "File_paths=[]\n",
    "if path.isfile(Settings_File) == True:\n",
    "    with open (Settings_File, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            File_paths.append(line.rstrip().split(':')[1])\n",
    "        file.close()\n",
    "       \n",
    "    #Name of the reference specie we are going to use\n",
    "    Reference_Specie = File_paths[0]\n",
    "    Reference_Specie_short = Reference_Specie.replace('_proteins','')\n",
    "    #File called Duplications.tsv\n",
    "    Duplications_File = File_paths[1]\n",
    "    #Species labeled and non labeled trees\n",
    "    Species_Tree = File_paths[2]\n",
    "    Non_labeled_tree = File_paths[3]\n",
    "    #Folder containing all the files of the Orthologues of the reference specie\n",
    "    Orthologues_Folder = File_paths[4]\n",
    "    #All the proteins of the reference specie used\n",
    "    Proteins_Fasta = File_paths[5]\n",
    "    #Files containing the orthogroups (Orthogroups.tsv) and the orthogroups of the Unassigned genes(Orthogroups_UnassignedGenes.tsv)\n",
    "    Orthogroups_File = File_paths[6]\n",
    "    Orthogroup_UnassignedGenes = File_paths[7]\n",
    "    Output = File_paths[8]\n",
    "    Support = float(File_paths[9])\n",
    "\n",
    "    \n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------#\n",
    "    #DUPLICATIONS\n",
    "    #-----------------------------------------------------------------------------------------------#\n",
    "\n",
    "    #DataFrame showing all the duplications provided by OrthoFinder.\n",
    "    DUPLICATIONS_DF = pd.read_csv(Duplications_File, delimiter = \"\\t\", header=0)\n",
    "\n",
    "    ###############################################################################################################################################################\n",
    "    #Functions:\n",
    "    #With this function I am trying to get rid of all those duplicated genes in a Clade. It also renames the gene names in orden to have them all in the same format.\n",
    "    def extract_duplication_events (Clade_duplication_table):\n",
    "        Total_dupl_genes = []\n",
    "        Genes_1 = []\n",
    "        Genes_2 = []\n",
    "        for gene in Clade_duplication_table['Genes 1']:\n",
    "            Genes_1.append(gene)\n",
    "        for gene in Clade_duplication_table['Genes 2']:\n",
    "            Genes_2.append(gene)\n",
    "        Genes_1 = ','.join(Genes_1)\n",
    "        Genes_1 = Genes_1.replace(' ','')\n",
    "        Genes_2 = ','.join(Genes_2)\n",
    "        Genes_2 = Genes_2.replace(' ','')\n",
    "        Unique_Genes = Genes_1.split(',') + Genes_2.split(',')\n",
    "        for gene in Unique_Genes:\n",
    "            if Reference_Specie in gene:\n",
    "                gene = gene.replace(Reference_Specie, '')\n",
    "                gene = gene[1:]\n",
    "                if gene not in Total_dupl_genes:\n",
    "                    Total_dupl_genes.append(gene)\n",
    "        return Total_dupl_genes\n",
    "\n",
    "    #This function is used to obtain the species names from the oldest one to our Reference_Specie\n",
    "    def get_subtree (Reference_Specie, Tree):\n",
    "        Clades = []\n",
    "        for i in range(0,2):\n",
    "            if Reference_Specie not in Tree[i].get_leaf_names():\n",
    "                Clades.append(Tree[i].get_leaf_names())\n",
    "            else:\n",
    "                Subtree = Tree[i].get_children()\n",
    "        return Clades, Subtree\n",
    "\n",
    "    #The following function is used to shorten the names of the species so it is easier to read them.\n",
    "    #Función para dejar los nombres de las especies sin adornos -> Cambiar según los nombres que le hayas puesto a las especies, se pueden añadir o quitar ifs y si se usa la misma estructura que tienen estos no debería haber problema.\n",
    "    def short_names (Species):\n",
    "        Clades_short = []\n",
    "        for species in Species:\n",
    "            if '_proteins' in species:\n",
    "                Clades_short.append(species.replace('_proteins', ''))\n",
    "            if 'v2' in species:\n",
    "                Clades_short.remove(species.replace('_proteins', ''))\n",
    "                Clades_short.append(species.replace('_v2_proteins', ''))\n",
    "            if 'v3' in species:\n",
    "                Clades_short.remove(species.replace('_proteins', ''))\n",
    "                Clades_short.append(species.replace('_v3_proteins', ''))\n",
    "        return Clades_short\n",
    "\n",
    "    #################################################################################################################################################################\n",
    "    #We are going to get the clades we are interested in (related to the reference specie) directly from the Species_Tree\n",
    "    S_tree = Phylo.read(Species_Tree, \"newick\")\n",
    "\n",
    "    Tree_Path = []\n",
    "    for clade in S_tree.trace('N0', Reference_Specie):\n",
    "        c = str(clade)\n",
    "        Tree_Path.append(c)\n",
    "    Tree_Path = list(reversed(Tree_Path))\n",
    "    Tree_Path.remove(Reference_Specie)\n",
    "    #Para poder buscar las Clades en el árbol necesitas que Tree_Path no contenga la especie referencia. Se vuelve a meter después.\n",
    "\n",
    "    #Names from the species tree -> With this we obtain a list of lists (if some species are agrupated they will be inside a list)\n",
    "    Tree = ete3.Tree(Non_labeled_tree)\n",
    "    Clades = []\n",
    "    for i in range(0,len(Tree_Path)):\n",
    "        if i == 0:\n",
    "            Clades += get_subtree(Reference_Specie, Tree.get_children())[0]\n",
    "            Subtree = get_subtree(Reference_Specie, Tree.get_children())[1]\n",
    "        else:\n",
    "            Clades += get_subtree(Reference_Specie, Subtree)[0]\n",
    "            Subtree = get_subtree(Reference_Specie, Subtree)[1]\n",
    "    Clades=list(reversed(Clades))\n",
    "\n",
    "    #It is useful though to have all the species names in just one list:        \n",
    "    Clades_Only = []\n",
    "    for element in Clades:\n",
    "        if len(element) == 1:\n",
    "            Clades_Only += element\n",
    "        else:\n",
    "            for clade in element:\n",
    "                Clades_Only.append(clade)\n",
    "    Clades_Only = short_names(Clades_Only)\n",
    "\n",
    "    Species_Path = []\n",
    "    for element in Clades:\n",
    "        if len(element) == 1:\n",
    "            Species_Path += short_names(element)\n",
    "        if len(element) > 1:\n",
    "            Species_Path.append(element[1].replace('_proteins','') + '_group')\n",
    "\n",
    "    #This code is used to now the node in wich the novo gene is originated\n",
    "    #Poner en el diccionario el easy_node\n",
    "    Clade_Path = {Reference_Specie_short:Reference_Specie_short}\n",
    "    for i in range(0,len(Tree_Path)):\n",
    "        Clade_Path[Species_Path[i]] = Tree_Path[i]\n",
    "\n",
    "    #Para los siguientes pasos necesitas que la especie referencia esté en Tree_Path, para que salga ordenado tienes que girar la lista.\n",
    "    Tree_Path = list(reversed(Tree_Path))\n",
    "    Tree_Path.append(Reference_Specie)\n",
    "    Tree_Path = list(reversed(Tree_Path))\n",
    "\n",
    "    #I am obtaining the duplicated genes from each Clade (all repetitions inside each Clade are already eliminated)\n",
    "    Duplications_by_Clade = []\n",
    "    for i in range(0,len(Tree_Path)):\n",
    "            duplication_events_table = DUPLICATIONS_DF[(DUPLICATIONS_DF['Species Tree Node'] == Tree_Path[i]) & (DUPLICATIONS_DF['Support'] >= Support)] \n",
    "            Duplications_by_Clade.append(extract_duplication_events(duplication_events_table))\n",
    "\n",
    "    #Now I am comparing each Clade with each other in order to eliminate in between clades repetitions. As it is sorted from newer duplications to older the newest duplicated genes will be kept and the older one will be removed\n",
    "    Copies = []\n",
    "    for i in range(0,len(Duplications_by_Clade)):\n",
    "        for z in range (1,len(Duplications_by_Clade)):\n",
    "            for elemento in Duplications_by_Clade[z]:\n",
    "                if elemento in Duplications_by_Clade[i]:\n",
    "                    Copies.append(elemento)\n",
    "\n",
    "    for i in range(1,len(Duplications_by_Clade)):\n",
    "            for z in range (0,len(Duplications_by_Clade[i])):\n",
    "                if Copies[z] in Duplications_by_Clade[i]:\n",
    "                    Duplications_by_Clade[i].remove(Copies[z])\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "    #Here we are getting the duplication events in each node. \n",
    "    Duplication_events = []\n",
    "    for i in range(0,len(Tree_Path)):\n",
    "        table = DUPLICATIONS_DF[(DUPLICATIONS_DF['Species Tree Node'] == Tree_Path[i]) & (DUPLICATIONS_DF['Support'] >= Support)]\n",
    "        Duplication_events.append(len(table))\n",
    "\n",
    "    #It is important to be certain about the duplication events of the reference specie, that is why we are calculating that number in a diferent way:\n",
    "    Reference_Specie_DF = DUPLICATIONS_DF[(DUPLICATIONS_DF['Species Tree Node'] == Reference_Specie) & (DUPLICATIONS_DF['Support'] >= Support)] \n",
    "\n",
    "    Orthogroups_copies = []\n",
    "    for Orthogroup in Reference_Specie_DF['Orthogroup']:\n",
    "        Orthogroups_copies.append(Orthogroup)\n",
    "\n",
    "    Orthogroups = []    \n",
    "    for Orthogroup in Orthogroups_copies:\n",
    "        if Orthogroup not in Orthogroups:\n",
    "            Orthogroups.append(Orthogroup)\n",
    "\n",
    "    Total_genes_Orthogroup = []\n",
    "    for Orthogroup in Orthogroups:\n",
    "        Genes_Orthogroup = []\n",
    "        ORTHOGROUP_DF = Reference_Specie_DF[(Reference_Specie_DF['Orthogroup'] == Orthogroup)]\n",
    "        for i in range(0,len(ORTHOGROUP_DF)):\n",
    "            Genes_Orthogroup += ((ORTHOGROUP_DF.iloc[i]['Genes 1'].replace(' ','') + ',' + ORTHOGROUP_DF.iloc[i]['Genes 2'].replace(' ','')).split(','))\n",
    "        Total_genes_Orthogroup.append(Genes_Orthogroup) \n",
    "\n",
    "    Non_duplicated_genes_Orthogroup=[]\n",
    "    for element in Total_genes_Orthogroup:\n",
    "        Non_duplicated_genes_Orthogroup.append(list(set(element)))\n",
    "\n",
    "    Duplication_Events_Ref_Spe = 0\n",
    "    for element in Non_duplicated_genes_Orthogroup:\n",
    "        Duplication_Events_Ref_Spe += (len(element)-1)        \n",
    "\n",
    "    #In order to normalize the duplication events we need to now the branch length of each node.\n",
    "    Branch_Length = {}\n",
    "    for node in S_tree.find_clades(branch_length = True):\n",
    "        Branch_Length[node.name] = node.branch_length\n",
    "\n",
    "    ##########################################################################################################################################################################################    \n",
    "    #Creating a tsv file to save all the duplications of each node\n",
    "    Clade_Duplications = {}\n",
    "    for i in range(0,len(Tree_Path)):\n",
    "        if '_proteins' in Tree_Path[i]:\n",
    "            Clade_Duplications[Tree_Path[i].replace('_proteins', '')] = Duplications_by_Clade[i]\n",
    "        else:\n",
    "            Clade_Duplications[Tree_Path[i]] = Duplications_by_Clade[i]\n",
    "\n",
    "    Duplications_file = open(str(date.today()) + '-' + Output + \"_Duplication_Analysis.tsv\", \"w\")\n",
    "\n",
    "    writer = csv.writer(Duplications_file, delimiter = '\\t')\n",
    "    writer.writerow(['Node','Genes'])\n",
    "    for key, value in Clade_Duplications.items():\n",
    "        for gene in value:\n",
    "            writer.writerow([key, gene])\n",
    "\n",
    "    Duplications_file.close()\n",
    "\n",
    "    ##########################################################################################################################################################################################\n",
    "    #Creating a tsv file to save the overall information of the Gene Duplications Analysis\n",
    "    Duplications_Overall = open(str(date.today()) + '-' + Output + \"_Duplication_Overall.tsv\", \"w\")\n",
    "\n",
    "    writer = csv.writer(Duplications_Overall, delimiter = '\\t')\n",
    "\n",
    "    counter = 0\n",
    "    writer.writerow(['Node','Specie','Duplications', 'Dupl Events', 'Branch length', 'Normalized events'])\n",
    "    for key, value in Clade_Duplications.items():\n",
    "        if Reference_Specie_short in key:\n",
    "            writer.writerow([key, list(Clade_Path.keys())[counter], len(value),Duplication_Events_Ref_Spe, Branch_Length.get(key + '_proteins'),(Duplication_Events_Ref_Spe)/(Branch_Length.get(key+'_proteins')*1000)])  \n",
    "        if key not in Branch_Length.keys() and Reference_Specie_short not in key:\n",
    "            continue\n",
    "        elif Reference_Specie_short not in key:\n",
    "            writer.writerow([key, list(Clade_Path.keys())[counter],len(value),Duplication_events[counter], Branch_Length.get(key),(Duplication_events[counter])/(Branch_Length.get(key)*1000)])\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    Duplications_Overall.close()\n",
    "\n",
    "    print(\"Analysis done, new file: \" + str(date.today()) + '-' + Output + \"_Duplication_Analysis.tsv\")\n",
    "    print(\"Analysis done, new file: \" + str(date.today()) + '-' + Output + \"_Duplication_Overall.tsv\")\n",
    "\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------#\n",
    "    #DE NOVO\n",
    "    #------------------------------------------------------------------------------------------------------------#\n",
    "    ###########################################################################################################################################################################################\n",
    "    #Function:\n",
    "    #The following function is used to transform a DataFrame into a dictionary(key=Gene: value=Orthogroup)\n",
    "    #Te coge un dataframe y te lo transforma en un diccionario del tipo -> Gen:Orthogroup\n",
    "    def orthogroups_todict (DataFrame, Species):\n",
    "        Orthogroup_Specie = DataFrame[['Orthogroup',Species]].dropna()\n",
    "        Orthogroups_DICT = Orthogroup_Specie.set_index('Orthogroup')[Species].to_dict()\n",
    "        Orthogroup_Genes = {}\n",
    "        for element in Orthogroups_DICT:\n",
    "            Orthogroup_Genes[element] = Orthogroups_DICT[element].replace(' ','').split(',')\n",
    "        Gene_Orthogroup = {}\n",
    "        for element in Orthogroup_Genes:\n",
    "            for gene in Orthogroup_Genes.get(element):\n",
    "                Gene_Orthogroup[gene] = element\n",
    "        return Gene_Orthogroup\n",
    "\n",
    "    ##############################################################################################################################################################################################\n",
    "\n",
    "    #First we transform all the files in the Orthologues folder into DataFrames\n",
    "    #Obtener todos los archivos de Orthologos y guardarlos en una lista de DataFrames\n",
    "    Orthologues_Files = glob.glob(Orthologues_Folder)\n",
    "\n",
    "    Orthologues_Tables = []\n",
    "    for file in Orthologues_Files:\n",
    "        data = pd.read_csv(file, sep = '\\t')\n",
    "        Orthologues_Tables.append(data)\n",
    "\n",
    "    #Here we are obtaining all the orthologues in the dataframe and saving them in lists.    \n",
    "    #Lista de listas. En cada lista hay los Ortologos de genes de la especie referencia de cada especie\n",
    "    Orthologues = []\n",
    "    for table in Orthologues_Tables:\n",
    "        Group_genes=[]\n",
    "        for gene in table[Reference_Specie]:\n",
    "            Group_genes.append(gene.replace(' ',''))\n",
    "        Total_genes = ','.join(Group_genes)\n",
    "        Group_genes = Total_genes.split(',')\n",
    "        Orthologues.append(Group_genes)\n",
    "\n",
    "    #We are going to get the names of the species from the Orthologues_Folder.\n",
    "    #Lista con el nombre de las especies (el orden sigue el orden de los DataFrames). Se usa porque está en el mismo orden que la lista de listas con los ortologos y así se hace el diccionario\n",
    "\n",
    "    #Names from the Orthologues_Folder\n",
    "    Species_Names = []\n",
    "    for table in Orthologues_Tables:\n",
    "        Species_Names.append(table.columns[2])\n",
    "    Species_Names=short_names(Species_Names)\n",
    "\n",
    "    #Para los siguientes pasos necesitas que está lista no contenga la especie referencia\n",
    "    Tree_Path.remove(Reference_Specie)\n",
    "\n",
    "    #Here we are creating a dictionary in order to now wich Orthologues are from what specie. We are interested in getting the lists of Orthologues ordered according to the tree path\n",
    "    #Hacer diccionario con las especies, como el orden es el de las orthologues_tables, se usa Species_names y no Clades.\n",
    "    Species_Genes = {}\n",
    "    for i in range(0,len(Species_Names)):\n",
    "        Species_Genes[Species_Names[i]] = Orthologues[i]\n",
    "    #A partir del diccionario podemos tener una lista de los genes ordenados por especie. Esto se usa para saber que genes de la especie de referencia son únicos\n",
    "    Orthologues_ordered_list = []\n",
    "    for i in range(0,len(Clades_Only)):\n",
    "        Orthologues_ordered_list += Species_Genes.get(Clades_Only[i])\n",
    "\n",
    "    #Once we have all the orthologues we are interested in knowing wich ones are exclusive of the reference specie. First we have to get all the proteins used in the study.\n",
    "    #Obtener una lista de todas las proteinas de la especie de referencia utilizadas en el estudio\n",
    "    All_genes=[]\n",
    "    for record in SeqIO.parse(Proteins_Fasta, \"fasta\"):\n",
    "        All_genes.append(record.id)\n",
    "\n",
    "    #Here we compare all the proteins used with those we have inside an Orthogroup and as a result we have the proteins exclusive of the reference specie.\n",
    "    #Lista en la que están las proteinas de novo en Scer. \n",
    "    ReferenceSpecie_de_novo = []\n",
    "    for genes in All_genes:\n",
    "        if genes not in Orthologues_ordered_list:\n",
    "            ReferenceSpecie_de_novo.append(genes)\n",
    "\n",
    "    #Once the reference specie is done we have to sort de novo genes by specie.   \n",
    "    #Lista con los genes de novo (no Especie de referencia). Cada lista dentro de la lista contiene los genes de una especie, ordenado según el árbol filogenético.\n",
    "    nonExclusive_de_novo = []\n",
    "    counter = 0\n",
    "    for i in range(0,len(Clades_Only)):\n",
    "        Species_de_novo = []\n",
    "        counter += len(Species_Genes.get(Clades_Only[i]))\n",
    "        for gene in Species_Genes.get(Clades_Only[i]):\n",
    "            if gene not in Orthologues_ordered_list[counter:]:\n",
    "                Species_de_novo.append(gene)\n",
    "        nonExclusive_de_novo.append(Species_de_novo) \n",
    "\n",
    "    #Diccionario --> Especie: Genes de novo\n",
    "    De_novo_genes = {}\n",
    "    De_novo_genes[Reference_Specie_short] = ReferenceSpecie_de_novo\n",
    "    for i in range(0,len(Clades_Only)):\n",
    "        De_novo_genes[Clades_Only[i]] = nonExclusive_de_novo[i]\n",
    "\n",
    "    #Now we want to have every de novo protein idexed by specie and by Orthogroup. In order to do that we need all the Orthogroups provided by OrthoFinder\n",
    "    #Extraer los Ortogrupos de las proteínas de todas las especies\n",
    "    Orthogroups_non_exclusive = pd.read_csv(Orthogroups_File, delimiter = \"\\t\", header=0)\n",
    "    #Extraer Orthogrupos exclusivos de la especie de referencia.\n",
    "    Orthogroups_Exclusive = pd.read_csv(Orthogroup_UnassignedGenes, delimiter = \"\\t\", header=0)\n",
    "\n",
    "    #Here we are using orthogroups_todict to get each protein related to an Orthogroup.\n",
    "    Orthogroups_non_exclusive_dict = orthogroups_todict(Orthogroups_non_exclusive, Reference_Specie)\n",
    "    Orthogroups_exclusive_dict = orthogroups_todict(Orthogroups_Exclusive, Reference_Specie)\n",
    "\n",
    "    #In order to have all the information of the protein in just one object we are going to create a dictionary (key = protein : values = [Orthogroup, Specie, Node])\n",
    "    De_novo_gene_information = {}\n",
    "    for specie in De_novo_genes:\n",
    "        for gene in De_novo_genes.get(specie):\n",
    "            if gene in list(Orthogroups_non_exclusive_dict.keys()):\n",
    "                De_novo_gene_information[gene] = [Orthogroups_non_exclusive_dict.get(gene), specie]\n",
    "            elif gene in list(Orthogroups_exclusive_dict.keys()):\n",
    "                De_novo_gene_information[gene] = [Orthogroups_exclusive_dict.get(gene), specie]\n",
    "\n",
    "\n",
    "    #Las especies agrupadas pasan a llamarse todas con un único nombre.\n",
    "    #Si hay más de una agrupación se llamaran el nombre que queramos + un número (empezando con 1 la agrupación más cercana a la especie de referencia)\n",
    "    for element in Clades:\n",
    "        if len(element) > 1:\n",
    "            for specie in De_novo_gene_information:\n",
    "                if De_novo_gene_information[specie][1] in short_names(element):\n",
    "                    De_novo_gene_information[specie][1] = element[1].replace('_proteins', '') + '_group'\n",
    "\n",
    "    for element in De_novo_gene_information:\n",
    "        De_novo_gene_information[element].append(Clade_Path[De_novo_gene_information[element][1]])\n",
    "\n",
    "    #In order to obtain the number of 'de novo' events we are going to count the number of Orthogroups    \n",
    "    Species_Path = list(reversed(Species_Path))\n",
    "    Species_Path.append(Reference_Specie_short)\n",
    "    Species_Path = list(reversed(Species_Path))\n",
    "\n",
    "    count_Orthogroups = []\n",
    "    for i in range(0, len(Species_Path)):\n",
    "        Orthogroup = []\n",
    "        for gene in De_novo_gene_information:\n",
    "            if De_novo_gene_information[gene][1] == Species_Path[i]:\n",
    "                if De_novo_gene_information[gene][0] not in Orthogroup:\n",
    "                    Orthogroup.append(De_novo_gene_information[gene][0])\n",
    "        count_Orthogroups.append(len(Orthogroup))\n",
    "\n",
    "    #############################################################################################################################################################   \n",
    "    #As an output we are going to get two files: one showing all \"de novo\" proteins and their information and another showing the sum up of this information. \n",
    "    #Archivo con todos los genes de novo, Orthogrupos y nodos\n",
    "    #Creating a tsv file to save all the duplications of each node\n",
    "    De_novo_file = open(str(date.today()) + '-' + Output + \"_De_novo_Analysis.tsv\", \"w\")\n",
    "\n",
    "    writer = csv.writer(De_novo_file, delimiter = '\\t')\n",
    "    writer.writerow(['Genes','Orthogroup', 'Clade', 'Node'])\n",
    "    for key, value in De_novo_gene_information.items():\n",
    "        writer.writerow([key, value[0], value[1], value[2]])\n",
    "\n",
    "    De_novo_file.close()\n",
    "\n",
    "    #############################################################################################################################################################\n",
    "    #File containing the overall 'de novo' information\n",
    "    De_novo_Overall = open(str(date.today()) + '-' + Output + \"_De_novo_Overall.tsv\", \"w\")\n",
    "\n",
    "    writer = csv.writer(De_novo_Overall, delimiter = '\\t')\n",
    "    writer.writerow(['Node','Specie','De novo genes', 'De novo events', 'Branch length', 'Normalized events'])\n",
    "\n",
    "    #Esta i es para coger el número de count_Orthogroups que toca\n",
    "    i = 0\n",
    "    for specie in Clade_Path:\n",
    "        #El counter te da el número de genes de novo\n",
    "        counter = 0 \n",
    "        for element in De_novo_gene_information:\n",
    "            if De_novo_gene_information[element][1] == specie:\n",
    "                counter += 1  \n",
    "        if Reference_Specie_short in Species_Path[i]:\n",
    "            writer.writerow([specie,specie,counter,count_Orthogroups[i],Branch_Length.get(Clade_Path.get(Species_Path[i])+'_proteins'), count_Orthogroups[i]/(Branch_Length.get(Clade_Path.get(Species_Path[i])+'_proteins')*1000)])\n",
    "        if Branch_Length.get(Clade_Path.get(Species_Path[i])) is None and i != 0:\n",
    "            continue\n",
    "        elif Branch_Length.get(Clade_Path.get(Species_Path[i])) is not None:\n",
    "            writer.writerow([Tree_Path[i-1],specie,counter,count_Orthogroups[i],Branch_Length.get(Clade_Path.get(Species_Path[i])),count_Orthogroups[i]/(Branch_Length.get(Clade_Path.get(Species_Path[i]))*1000)])\n",
    "        i += 1\n",
    "    De_novo_Overall.close()    \n",
    "\n",
    "    print('Analysis done, new file: ', str(date.today()) + '-' + Output + \"_De_novo_Analysis.tsv\")\n",
    "    print('Analysis done, new file: ', str(date.today()) + '-' + Output + \"_De_novo_Overall.tsv\")\n",
    "\n",
    "else:\n",
    "    Configurations = ['Reference_Specie (as the OrthoFinder named it -> ex:s_cerevisiae_proteins):', 'Duplications.tsv:', 'Species_Tree:', 'Non_labeled_Species_tree:', 'Orthologues_folder (all the Orthologues.tsv of the specie we are interested in as path/s_cerevisiae*.tsv):', 'Proteins_fasta(all the proteins used of the specie we are interested in):', 'Orthogroups_file(Orthogroups.tsv):', 'Orthogroup_UnassignedGenes.tsv:', 'Output(each file is named as for ex. Duplication_Overall_Output.tsv):', 'Support:']\n",
    "    with open ('settings.txt','w') as file:\n",
    "        file.writelines(\"%s\\n\" % i for i in Configurations)\n",
    "    print('settings.txt file created, needs to be filled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
